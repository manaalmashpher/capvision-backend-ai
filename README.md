# CapVision Backend AI

An AI-powered image captioning system that generates detailed descriptions of images and converts them to speech using advanced neural networks and Google Cloud services.

## ğŸš€ Features

- **Neural Image Captioning**: Uses a CNN-RNN architecture with Bahdanau attention mechanism
- **Dual Caption Generation**:
  - Custom trained neural network model
  - Google Gemini AI integration for enhanced captions
- **Text-to-Speech**: Converts generated captions to audio using Google Cloud Text-to-Speech
- **RESTful API**: Flask-based web service with easy-to-use endpoints
- **Real-time Processing**: Fast image processing and caption generation

## ğŸ—ï¸ Architecture

The system consists of three main components:

1. **CNN Encoder**: Extracts visual features from images using InceptionV3
2. **RNN Decoder**: Generates captions using GRU with Bahdanau attention
3. **TTS Integration**: Converts text to speech using Google Cloud services

## ğŸ“‹ Prerequisites

- Python 3.8+
- TensorFlow 2.x
- Google Cloud account with Text-to-Speech API enabled
- Google AI API key for Gemini integration

## ğŸ› ï¸ Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/manaalmashpher/capvision-backend-ai.git
   cd capvision-backend-ai
   ```

2. **Create a virtual environment**

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Set up Google Cloud credentials**

   - Download your service account key from Google Cloud Console
   - Save it as `credentials.json` in the project root
   - Enable Text-to-Speech API in your Google Cloud project

5. **Configure environment variables**
   Create a `.env` file in the project root:
   ```env
   GEMINI_API_KEY=your_gemini_api_key_here
   ```

## ğŸš€ Usage

### Starting the Server

```bash
python app.py
```

The server will start on `http://localhost:5000`

### API Endpoints

#### 1. Neural Network Caption Generation

**POST** `/upload`

Generates captions using the custom trained neural network model.

**Request:**

- Method: POST
- Content-Type: multipart/form-data
- Body: image file

**Response:**

```json
{
  "caption": "A detailed description of the image",
  "audio_url": "/get_audio"
}
```

**Example using curl:**

```bash
curl -X POST -F "file=@your_image.jpg" http://localhost:5000/upload
```

#### 2. Gemini AI Caption Generation

**POST** `/gemini_caption`

Generates captions using Google's Gemini AI model for enhanced descriptions.

**Request:**

- Method: POST
- Content-Type: multipart/form-data
- Body: image file

**Response:**

```json
{
  "caption": "A detailed description generated by Gemini AI",
  "audio_url": "/get_audio"
}
```

**Example using curl:**

```bash
curl -X POST -F "file=@your_image.jpg" http://localhost:5000/gemini_caption
```

#### 3. Audio Retrieval

**GET** `/get_audio`

Retrieves the generated audio file (MP3 format).

**Response:**

- Content-Type: audio/mpeg
- Body: MP3 audio file

## ğŸ§  Model Architecture

### CNN Encoder

- **Base Model**: InceptionV3 (pre-trained on ImageNet)
- **Output**: 2048-dimensional feature vectors
- **Custom Dense Layer**: Projects features to embedding dimension (256)

### RNN Decoder

- **Architecture**: GRU with Bahdanau attention mechanism
- **Embedding Dimension**: 256
- **Hidden Units**: 512
- **Vocabulary Size**: 5001 (5000 words + 1 for padding)
- **Max Caption Length**: 40 words

### Attention Mechanism

- **Type**: Bahdanau attention
- **Purpose**: Focuses on relevant image regions while generating each word
- **Output**: Attention weights for visualization

## ğŸ“Š Training Data

The model was trained on the Flickr8k dataset with:

- 8,000 images with 5 captions each (40,000 total captions)
- Preprocessed captions with `<start>` and `<end>` tokens
- Images resized to 299x299 pixels
- InceptionV3 preprocessing applied
- Vocabulary size of 5,000 unique words

## ğŸ”§ Configuration

Key parameters can be modified in `modulecomponents/config.py`:

```python
EMBEDDING_DIM = 256    # Feature embedding dimension
UNITS = 512           # GRU hidden units
MAX_LENGTH = 40       # Maximum caption length
VOCAB_SIZE = 5001     # Vocabulary size (5000 words + 1 for padding)
```

## ğŸ“ Project Structure

```
capvision-backend-ai/
â”œâ”€â”€ app.py                          # Main Flask application
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ modulecomponents/               # Core modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                   # Configuration parameters
â”‚   â”œâ”€â”€ model_architecture.py       # Neural network models
â”‚   â””â”€â”€ preprocessing.py           # Data preprocessing utilities
â”œâ”€â”€ saved_models/                   # Pre-trained model files
â”‚   â”œâ”€â”€ encoder.keras
â”‚   â””â”€â”€ decoder.keras
â”œâ”€â”€ checkpoints/                    # Training checkpoints
â”œâ”€â”€ tokenizer.json                  # Text tokenizer
â”œâ”€â”€ neuralnet.ipynb                 # Training notebook
â””â”€â”€ test.py                         # Testing utilities
```

## ğŸ§ª Testing

Run the test script to verify model loading and basic functionality:

```bash
python test.py
```

## ğŸ”’ Security Notes

- Never commit `credentials.json` or API keys to version control
- Use environment variables for sensitive configuration
- Implement proper authentication for production deployments

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- TensorFlow team for the excellent deep learning framework
- Google Cloud for Text-to-Speech services
- Google AI for Gemini integration
- The open-source community for various libraries and tools

## ğŸ“ Support

If you encounter any issues or have questions, please:

1. Check the existing issues on GitHub
2. Create a new issue with detailed information
3. Contact the maintainers

**Made with â¤ï¸ for accessibility and AI innovation**
